from pyspark.sql import SparkSession
import pandas as pd
import numpy as np
from numpy import mean
from tqdm import tqdm


class Recommendation(int):
    def __init__(self, uid):
        self.uid = uid

    def pick_recommendation(self):
        if self.uid in testing_uid:
            result2 = spark.sql(
                'select movieId from result where rating is null and userId = ' + str(
                    self.uid) + ' order by prerating limit 5 ')
            result2.show()
        else:
            print('userId is not in the list')
            
            
def cos_sim(vector_a, vector_b):
    """
    计算两个向量之间的余弦相似度
    :param vector_a: 向量 a
    :param vector_b: 向量 b
    :return: sim
    """
    vector_a = vector_a.values
    vector_b = vector_b.values
    vector_a = np.mat(vector_a)
    vector_b = np.mat(vector_b)
    num = float(vector_a * vector_b.T)
    denom = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)
    cos = num / denom
    # sim = 0.5 + 0.5 * cos
    return cos


def rmse(predictions, targets):
    result = np.sqrt(((np.array(predictions) - np.array(targets)) ** 2).mean())
    return result


spark = SparkSession.builder.appName('itemcf').getOrCreate()  # project name

#read ratings
ratings = spark.read.csv('ml-25m/ratings.csv', inferSchema=True, header=True)
ratings.createOrReplaceTempView("ratings")
spark.sql("select count(userId) as num from ratings").show()
ratings_s = spark.sql("select * from ratings limit 2000")
te = ratings_s
te_df = ratings_s.toPandas()
# (te, ho) = ratings.randomSplit([0.0001, 0.9999])
te.createOrReplaceTempView("te")
user = spark.sql("select distinct userid from te")
movies = spark.sql("select distinct movieid from te")
user = user.toPandas()
movies = movies.toPandas()
# print(len(movies))
user = user['userid'].values.tolist()
movies = movies['movieid'].values.tolist()

test_p = te_df.drop(['timestamp'], axis=1)
test = pd.pivot_table(test_p, index=['userId'], columns=['movieId'], values=['rating'])
test.columns = movies
df = test.replace(np.nan, 0)
training = df.sample(frac=0.8)
testing = df[~df.index.isin(training.index)]
matrix = pd.DataFrame(columns=movies, index=movies)
n = 0

for j in tqdm(range(len(movies))):
    for i in range(n, len(movies), 1):
        matrix.loc[movies[j], movies[i]] = cos_sim(training[movies[j]], training[movies[i]])
        # print(cos_sim(df[movies[j]], df[movies[i]]))
    n = n+1
matrix = matrix.replace(np.nan, 0)
matrix_v = matrix.values
matrix_v = matrix_v + matrix_v.T-np.diag(matrix_v.diagonal())
matrix = pd.DataFrame(matrix_v)
testing_r = testing.reset_index().drop(['userId'], axis=1)
testing = testing.reset_index()
testing_uid = testing.userId.tolist()   ###testing###
prediction_res = testing    ###testing###
matrix_r = matrix.reset_index()

rmse_list = []
for i in range(len(testing_r.index)):
    a = testing_r.iloc[i].tolist()
    a_zero = testing_r.iloc[i].tolist()
    list_zero = []
    list_notzero = []
    for j in range(len(a)):
        if a[j] != 0:
            print(j)
            break
        else:
            continue
    b = matrix_r.iloc[j, 1::].tolist()
    c = [i * a[j] for i in b]
    prediction = [i * a[j] for i in b]
    for k in range(len(a)-1,-1,-1):
        if a[k] == 0:
            list_zero.append(k)
            del a[k]
            del c[k]
        else:
            list_notzero.append(k)
            continue
    # for t in range(len(list_notzero)):
    #     del prediction[list_notzero[t]]
    prediction.insert(0, int(testing_uid[i]))
    prediction_res.loc[i] = prediction
    rmse_r = rmse(c, a)
    rmse_list.append(rmse_r)
avg_rmse = mean(rmse_list)
prediction_res[['userId']] = prediction_res[['userId']].astype(int)
prediction_res_T = prediction_res.set_index('userId').stack().reset_index()
prediction_res_T.columns = ['userId', 'movieId', 'rating']
spark = SparkSession.builder.getOrCreate()
prediction_res_T_spark = spark.createDataFrame(prediction_res_T)
prediction_res_T_spark.createOrReplaceTempView('predictionRTS')
#
# resultt = spark.sql('select userId, movieId, rating from ratings where rating = 0')
# resultt.show()
#
result = spark.sql('select p.userId, p.movieId, r.rating, round(p.rating, 1) as prerating from ratings as r right join predictionRTS as p'
                   ' on r.userId = p.userId and r.movieId = p.movieId')
# result.show()
result.createOrReplaceTempView('result')
result2 = spark.sql('select movieId from result where rating is null and userId = 4 order by prerating limit 5 ')
# dtest = result.toPandas()
result2.show()
test_T = test.reset_index().set_index('userId').stack().reset_index()






